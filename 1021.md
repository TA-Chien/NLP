# 1021 :baby: 
## Text Classification :+1: 
* Assign  objects from a universe to two or more classes 
* We can try not to do Feature extraction with Artificial Neural Network
### Bag of Word Representation
* The presence or absense of each word is denote by the value of the cell (according to dictionary)
* Loss the order of the word

### Learning Models
#### Ngram, Naive Bayes, Tree, Logistic, NN, KNN
* Perceptrons is the single layer of NN model

* Naive Bayes - Prevent from zero probability 
    * Do additive (Laplace) smoothing

* Tree
    * Decision Tree
    * Random Forest (need to do pruning)
    * Gradient Tree Boosting : Improve the existing tree by adding a weak tree before generating the next tree ( Trees are generated in sequential )

* Perceptron : Find a line to separate spaces
    * A linear model
    * Single hidden layer
    * To compute n+1 parameters (W0 (bias), W1, W2,...) 
    * Avoid to go through the original point by bias term 

*  Logistic model : use sigmoid function for binary problem and softmax function for multiple problem.
    * doesn't rely on the assumption of independency
    * we can explore features without consideration it's dependencies.

* SVM : SVM is not usally used in NLP according to that its efficiency is much lower, but it really performs great in Maximum Margin topics.

* NN : can handle more complex relationship problems. ex: {good} ,  {not,good}. However, for the later should construct more than two layers to deal with the situation.

* K-NN : 
    * Use good similarity measurement. ex: Jaccard Coefficient, Cosine Similarity. 
#### Multi-class classification
* one against rest

### Feature Extraction
* Bag-of-word
    * can not distinguish the meaning that the order of two sentence is reverse becuase it consider this phenomenon to be identical.
* NN-based representation
    * CNN and RNN are capital of extracting the word order information
    * Word embedding representing each word to a dense vector.
* Linguistics Features
    * Phonetics Features
    * Character Features
    * Word Features
    * Word Grouping : Merge related words into a form
    * Hypernyms : Reduce the size of features to prevent from the sparsity issue
    * Part-of-Speech Tags (POS) tagging
    * Dependency Parse Tree : Capture the relationships among the words in a sentence
    * Constitution Parse Tree : Capture the construction of an article or a writing style 
    * Connectives : Capture the relation between sentences in an article
Nobody want to note???? :ghost:  :clown_face: :satellite: 