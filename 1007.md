# **1007**    

---

### **Probability Language Model**
* A probability distribution over sequence of words
* Context
    * The Preceding word
* Only the sequential relationship is taken into account
    * Prediction of the next word
* Lack of the syntactic information from the tree-like structure
* Traditional Application 
    * Optical character recognition
    * Speech recognition
    * Machine translation 
        * Measure which sentence will have a bigger chance to happen
    * Grammatical error correction 
*Train a language model based on a large data set*
* Powerful weapon to select next word
* Use the chain rule to measure the joint probability of composing of a sentence
*P(A,B,C)=P(A)P(B|A)P(C|A,B)*
![](https://i.imgur.com/jlZaZgH.png)
* The Sparsity of Long Sequence
    * Practically it will lead to a zero count
        *    Markov Assumption to solve the question 
        - Only focus on the NEW sequence 
        *Group the last few words to shorten the long sequence into bigram or trigram*
* The Unigram model : ![](https://i.imgur.com/G06BUn5.png)
* Summation of log probabilities are used in practice : ![](https://i.imgur.com/H7vur0R.png)
-- The value of log probability will not loss any accurate of results   *Linear vs. exponential curve*
* Unigram model sometimes will lead to a loss of information of word 
* Adoption of Bigram Model : ![](https://i.imgur.com/yvdctrG.png)
* Extension of N-gram Model : ![](https://i.imgur.com/zGsFT8T.png)
* Empirical probability of estimation of N-gram probability : **MLE**


---

### Neural Language Models

* The NN-based language model can easily handle the longer term relationships even the naive RNN
* Recurrent Neural Network : The output of before the step t have been passed to the coming step
    * Given the previous words then the model can generate a prediction of next word 
*Not take Markov assumption into consider*
* GPT : Language Model Based on the Transformer


---
### Smoothing
* Deal with the problem of zero probability
* Pretend the model has saw each word once 
* Add-one Smoothing : reserve too many mass for N-gram
* Held Out Estimator : Separate training set into 2 parts   
    * See how often ngrams that appear c times in the training set tend to turn up in the held out set. 
* ![](https://i.imgur.com/UZJWZyi.png) = Total number of bigram in theoretically - Total number of bigram in corpus
* Adjusted Counts : ![](https://i.imgur.com/icrQkVD.png) make us have a small amount of unseen words 
* Good-Turing Smoothing : Much more accurate than the method of smoothing 
    * ![](https://i.imgur.com/CERabHt.png) is the value of possible bigram
 
 
---
### How good of a Language Model
* Principle of evaluation will focus on the final application 
 *Choose a suitable one for the task*
* Perplexity : Split data into training and testing data than take the testing data to fit the model
    * The lower the perplexity, the better the language model 
