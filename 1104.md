
# 1104
## POS Tagging & Sequence Labeling

#### Overfitting 

* Too good for training data(lower error rate) bad at test data 
* Regularization, feature selection, reduce complexity medel are the ways to prevent Overfitting.
* Regularization : Key point of this week 
    * Especially in logistic regression or Multilayer perceptron 
    * Lasso : $L_{1}$ norm (Absolute)
        * punishes uniformly for both low and high values, and the result would be more sparse (many parameters are zero)
    * Ridge : $L_{2}$ norm (Square)
        *  punishes for high parameter weights
    * Elastic-Net : A combination of $L_{1}$ and $L_{2}$
    
* Decrease the depth / size hidden layers / Dropout (may be more useful) 
    * NN
* Feature selection
    *  Use Chi-square test to do feature selection ( between Xs and Y )
    * But feature selection by model may be more powerful despite that it is time-comsuming comparing to the other two methods.
    * The less the features we used is easier to interpret the model.


---
#### Part-of-Speech Tagging
* Input is a sentence , Output the POS-tag.
* Know Contextual meaning by understanding the rule 
* Input the sentences of n POS-tag
* Fix the problem of ambiguity of words 
* Information for POS-tag
  * Knowledge of contenxual
  * Distribution of words
  * Wordform
 * Applications 
     * Machine translations 
     * Text-to-speech
     * Providing fundamental information for syntactic/dependency parsers : The first step


---
#### Models for sequence labeling 
* Input is a sentence, Output is the sequence of POS tags for X
* We need to consider the information of its neighbor words.
    * Markov Model
        * First - ordered : state t+1 only depends on state t.
        * Second - ordered : state t+1 only depends on state t and t-1.
    * HMM
        * Transition Probabilities
        * Initial probabilities
        * Obeservation Probabilities
        * Parameter of HMM : current word and that of its previous word $a_{ij} = \frac{C(S_{i}S_{j})}{C(S_{i})}$ 
        * $b_{j}(w)$ The distribution of POS tags of a word.
        * $\delta_{t}(j) = a_{ij}b_{j}$
        * $\varphi_{t}(j)$ : Back checking 
    * MEMM : Consider all of X and prior y 

