# 1202
# Neural Network :hammer_and_pick: 

## Feed Forward Neural Network
*    Activation Function : Sigmoid and $tanh$ will not have a significant difference in large value
* Sigmoid : [0,1]
    * Suitable for output layer (Softmax for multiple outputs)
* tanh : [-1,1]
* ReLu : [0,x]
    * Suitable for hidden layers 
* Softmax for multi-nominal outputs 
    * Use maximun entropy loss 
* Sigmoid for two-nominal outputs
* Mini-batch SGD
* Back-propagation:update weights
    * Hard to capture the order of words 

## Convolution Neural Network
*    A sentence is represented as a matrix with the dimension of n * d,where n is number of words and d is the dimension of word embeddings
*    Filters can be regard as features 
*    We can capture the ordered features in embedding matrix
*    we can not assume the more adjacent word in a sentence is more important
*    In NLP can not only consider 1 pixel(2-D) for text classification (1-D)
*    Extracting neighboring word embeddings and converting to filtered representations 
*    Capture the N-gram as a hidden layer for filters
*    Dense Vector : Max-pooling operation takes the max over each dimension, resulting in a pooled vector
*    The output layer finally generates the probability of each category based on the pooled vector
*    DL models adapt the back propagation to optimize Feature extraction and Classification 
## Recurensive Neural Network


