# 1230
## Semi-supervised 

### Semi-supervised learning
*    Pseudo-labeled data
        * Large amount of self-labeled data is available on the Internet and usually used as training/test data for a wide range of NLP tasks.
        * Have a lot of data and company with lots of noise.
        * Self-training
* Pre-training
    * Pre-train sentence encoder : Generate a vector to represent the sentence of input 
    * Auto Encoder : Don't need to prepare the training data. 
        * The data is self-labeled 
* Data Augmentation
    * Generate a sentence based on original one 

* Back Translation
    * The quality is highly relied on the machine translation model.

* Co-Training
    * Arrange a numbers of tasks to train a model at the same time


### Multi-task Learning
* Use the Shared Layers and get the total Loss to train two models
* Can use in supervise and unsupervise learning
* 

## Parsing
### Syntactic Parsing
*    Syntax capture the information in a tree structure
*    Capture the relationship of long distance in a sentence 
*    Do not care about the meaning of a sentence 
### Constituent
*    Context Free: Given part of words in a sentence, which parsing will not be influenced by other words in a sentence 
* Major Phrase Types
    * S: whole sentence (S → NP VP)
    * NP: noun phrase (名詞片語)
    * PP: prepositional phrase (介繫詞片語)
    * VP: verb phrase (動詞片語)
    * AP: adjective phrase (形容詞片語)
        * Usually used to express spatial and temporal locations and other attributes

### Discourse Parsing :baguette_bread: 
*    Full Discourse Parsing
        * EDU segmentation
        * Tree construction
        * Relation recognition
        * Nuclearity labeling
