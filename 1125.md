# 1125 :fried_egg: 
# Word Embedding
### Dense Vectors
*    A vector for a word 
*    semantic similarity:two similar words share with same vectors
*    Skipgram most use
*    Fasttext capture the suffix or prefix of words, it is considered to be better than word2vec

## Word2Vec
*    Do not need labels. Only need lots of sentences or corpus
* Represent 50 - 1000 dimensions
### CBOW
*    Capture the relationship between target word and contextual  words
*    Use Context window to predict the target word
*    Objective function:$\sum_{t}^{T}\sum_{c}logp(w_{t}|w_{c})$
### SKIP-GRAM
* Use the current word to predict the surrounding words
* Objective function:$\sum_{t}^{T}\sum_{c}logp(w_{c}|w_{t})$
* T is a sentence
* c is a specific token


### Nagative Sampling
* It's important idea 
### Semantic Similarity of Word Embeddings
*    Cosine similarity : Measure the angle between 2 words, which is commonly better than the gggevaluation with L1-norm or L2-norm.
*    The WordSimilarity-353 Test Collection
*    Similarity not equal to Relatedness
### Linguistic Regularity
*    3CosAdd : cos($b^{*},b - a + a^{*}$)
*    3CosAdd will be dominated by a large vector
*    3CosMul : can get rid of the domination problems 

## FastText
*    Consider the subword information 
*    Useful in a character word based model 
*    Bilingual Word Representations : Train data in English and apply in Chinese 
## Parallel Corpus
* A corpus consists of translation pairs from two languages.
* Extend to Bilingual Word Embeddings with Shuffling
* Capture a relationship between a word and a sentence
* GIZA++:used for chinese and English word alignment.