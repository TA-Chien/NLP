# 1209 :baby_chick: 
## CNN model :t-rex: 
*    Set up the length as 100 in practical 
*    Padding : Fill the blank of slot 
*    
## RNN model :-1: 
*    The output of the last hidden layer can be used as a feature of the input data
*    The more close to step t, the more effective to the $y_t$.
*    obly one activaiton funciton : tanh function

## Bi-Directional RNN
*    Take both forward/backward contextual information at the same time

## LSTM 
* have three hidden layers, input layer, forget layer and output layer, which if different from the property of single layer of RNN
* train different weights for different importance
* if you want to train longer dependence of words, it us much better than RNN generally.

## GRU
* simplified LSTM
* can use a larger hidden sizes due to the usage of only about 75% mamory conparing to the LSTM under same circumstance.

## Attention Mechanism :mag: 
* considering every output of the sequence
* That is, have additional weights for every output

## conclusion
* Attentive GRU have the almost best performance in the tasks of ironic detection, sentiment analysis and the other task(?) about three year ago.
* In general, GRU model is much better than CNN model.

## Hierarchical Neural Network Models
*  Use upper and lower RNN to capture relation within sentence and document
:taco: is delicious !
* Sentence Encoder: The role of the lower RNN is the sentence encoder, which converts a sequence of words into a dense vector.
* Compare with word embedding that only can handle the word 
* Pre-Trained Sentence Encoder
    * ELMo : A general pre-trained sentence encoder
        * Based on BiLSTM
        * Assumption of LSTM : The more close to the target word, the more being affect by the word

## Transformer
*  Capture every word pair 
*  BERT: Bidirectional Encoder Representations from Transformers
*  A generalized NN model
*  For a same word which will not have a same word vector in a sentence
*  Character-based representation traditional/simplified Chinese
*  No Chinese word segmentation
